1. The worst case asymptotic performance for the sort provided is O(n^2). This is because, regardless of how the elements in the array are arranged, sort will have an index i run from 0 to n-2 and an index j run from 1 to n-1 for each i value respectively. The index j thus visits n-1 + n-2 + n-3... + 1 + 0 other indices in the array. In the worst case, the inner for loop will swap values as many times as it visits those values, resulting in 4n basic operations being performed (swap takes 3, comparing takes 1). The index i visits a total of n - 1 values in the array. The total of these two is equal to ((n-1+1)(4(n-1))/2)  which results in a value in the order of n^2. This means that the asymptotic complexity of sort is O(n^2).
The worst case asymptotic performance for the insertion sort is O(n^2) (Note: I am using the insertion sort algorithm provided in the lecture notes). This occurs when the array is sorted in the reverse order. In this case, the inner for loop does not break until j = 0, because a[j-1] will always be greater than k, the value of reference. At each iteration, insertion sort will place the reference value at the very front of the array, but it will also visit every index between the index of the reference value and the index 0, and compare the two values. This means that, for the index i = 1, insertion sort will make 1 comparison, comparing it to index 0. For index i = 2, insertion sort will make 2 comparisons, comparing it to the index 1 and the index 0. For the last index n, insertion sort will make n comparisons and place the smallest value (since the array is in reverse order) in the front. This totals to (n*(n-1)/2), which gives O(n) complexity.
Their worst case asymptotic performances are equal.
In the best case asymptotic performance for sort, sort will ALSO have asymptotic complexity of O(n^2). This is because, when calculating the asymptotic performance of sort, the same logic applies regardless of the contents or ordering of the array. The guard of the for loops inside sort are independent of how each element appears to its neighbors or to any reference value; they are only broken upon reaching a certain index. In the best case, the inner for loop will perform 0 assignations and all comparisons, resulting in n-i basic operations being performed. This inner loop still retains O(n) complexity. Therefore, the best, and worst, and for that matter, every case of sort has an asymptotic complexity of O(n^2).
The best case asymptotic performance for insertion sort is when the two arrays are already sorted. In the inner for loop of the insertion sort, one of the conditions is that a[j-1] > k. The integer J always starts out with a value of i, for whichever value i represents the iteration of the outer loop that is currently being processed. K is given the value a[i]. In a sorted list, a[i-1] < a[i] for all i in the bounds of the array that have a previous element; this is the definition of a sorted list. Thus, a[i-1] < a[i] and we can substitute j for i and K for a[i] because these values represent the first iteration of the inner for loop. We can see that a[j-1] < K, which contradicts the inner for loop guard of a[j-1] > K. This means the inner for loop will never run for any i in the bounds of the array that has a previous value. The absence of a previous value is not something we have to worry about since i starts at 1. Since the inner loop never executes, the only index we observe is the index i. This means that the best case asymptotic performance for insert sort is the number of iterations of i, which runs from 1->n-1, resulting in O(n) or linear complexity. 
Insertion sort has a faster best-case asymptotic complexity.
I would expect insertion sort to be faster because in a typical randomized case, the time taken would fall somewhere between the best-case and worst-case asymptotic performances. Since sort has the same performance regardless of the orientation of the array, the asymptotic performance of sort will always be O(n^2). However, in practice, insertion sort will likely not shift every element to the first index, which would be the only instance in which it mimics O(n^2) asymptotic complexity. One of the clauses of the loop invariant of the insertion sort is that for each i, all elements to the left of i are sorted. At each iteration, insertion sort places the value at the index i at the correct spot in this sorted group. This is likely to be somewhere between the first and last indices of all elements before I, simply because placing everything at the first index would mean the array is sorted in reverse order, and placing everything in the last index would mean the array is already sorted. Since those two possibilities are 2 out of the n! possible permutations of all numbers in the array (assuming no duplicates) simple probability theory states that the array is likely to NOT be one of these two orientations, leaving insertion sort to very likely be better than sort.


2. The worst case asymptotic complexity of sort is O(n^2) as described in problem 1. The worst case asymptotic complexity of Selection sort is O(n^2) as well. This is because Selection sort, for every iteration of the for loop, checks all the values between a given index i (representing the current iteration as well as the index of the array to start at) and the last index of the array. This means that the first iteration runs through all values between index i = 0 and the last index of the array, with each subsequent iteration increasing i by 1 until i is at the last index of the array. The number of comparisons will thus be n + n-1 + n-2 + n-3... + 1, giving n(n-1)/2 which gives O(n^2) complexity.
The worst case asymptotic complexities are equal
The best case asymptotic complexity of sort is O(n^2) as described in problem 1. The best case asymptotic complexity of selection sort is O(n^2) as well. This is because, even if the minimum value is found immediately after the index i, the only way to confirm this is to check the value at each index until the very last index, which it must do for each iteration until the iteration representing the last index. This gives the same number of comparisons as the worst-case asymptotic complexity described above, and thus the mathematics will be the same. The complexity will equal O(n^2).
The best case asymptotic complexities are equal
Selection sort is expected to behave faster than sort because of the number of assignations that regular sort must perform, a number that is significantly less in selection sort. Consider one iteration of the outer for loop of the  selection sort. Selection sort assigns the variable "indexOfMinimum" to its own index. It then iterates through the rest of the array, a total of n-i-1 elements, making one comparison each iteration (we'll throw out the -1 for readability and so as to not confuse anyone). If the value at a certain index is smaller than the value at index "indexOfMinimum", "indexOfMinimum" is then assigned to this index. This could happen anywhere between 0 and n-i times. Then, the value at indexOfMinimum is swapped with the value at the original index, i. This requires 3 assignations, as swapping the values of two array indices generally do. In total, this will be anywhere between 4 and n-i + 4 assignations, and a definite n-i comparisons.
Consider one iteration of the outer for loop of the given sort. The given sort iterates through the rest of the array starting at index i+1 and ending at index n-1, for a total of n-i-1 comparisons (again throw out -1). If the value at a certain index is smaller than the value at index i, the value at index i is swapped with this value, giving 3 assignations. The most amount of times this could happen is n-i times, each giving 3 assignations for a total of 3*(n-i) assignations. Add that to the total number of comparisons and one receives 4*(n-i) fundamental operations.
Since both of these given sorts have the same number of comparisons for a given iteration of the inner and outer loops, let us consider only the fundamental operations outside the comparisons. Selection sort will only perform an assignation if the "if" statement executes, and the regular sort will perform a swap if the "if" statement executes. The base of reference for the "if" statement is the value at index i for the regular sort, and the value at index "indexOfMinimum" for the selection sort. When the if statement is executed in selection sort at an index j, selection sort changes the indexOfMinimum so that the base of reference is now the value at j. When the if statement is executed in the regular sort at an index j, the regular sort swaps the value at i with the value at j so that the value at i is now the value at j. This means that in both cases, both sorts assign their base of reference to the same number. When the if statement does not execute, there are no assignations so the base of reference never changes. At the beginning of the iteration, they are assigned to the same value since IndexOfMinimum is assigned to i. Thus, their base of reference remains equal through every iteration of the inner loop, which means that the number of times the selection sort performs an assignation is the same number as the number of times the regular sort performs a swap. This is crucial because a swap takes 3 assignations while a single assignation represents 1. However, one must note that the selection sort performs one mandatory swap at the end and a mandatory assignation at the beginning, giving it 4 extra assignations. Thus, if the "if" statement executes at least twice, the selection will be faster than the regular sort. In general, then, one can rely on the selection sort to be faster than the regular sort in array of reasonable size.




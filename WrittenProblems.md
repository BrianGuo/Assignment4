1. The worst case asymptotic performance for the sort provided is O(n^2). This is because, regardless of how the elements in the array are arranged, sort will have an index i run from 0 to n-2 and an index j run from 1 to n-1 for each i value respectively. The index j thus visits n-1 + n-2 + n-3... + 1 + 0 other indices in the array. In the worst case, the inner for loop will swap values as many times as it visits those values, resulting in 4n basic operations being performed (swap takes 3, comparing takes 1). The index i visits a total of n - 1 values in the array. The total of these two is equal to ((n-1+1)(4(n-1))/2)  which results in a value in the order of n^2. This means that the asymptotic complexity of sort is O(n^2).
The worst case asymptotic performance for the insertion sort is O(n^2) (Note: I am using the insertion sort algorithm provided in the lecture notes). This occurs when the array is sorted in the reverse order. In this case, the inner for loop does not break until j = 0, because a[j-1] will always be greater than k, the value of reference. At each iteration, insertion sort will place the reference value at the very front of the array, but it will also visit every index between the index of the reference value and the index 0, and compare the two values. This means that, for the index i = 1, insertion sort will make 1 comparison, comparing it to index 0. For index i = 2, insertion sort will make 2 comparisons, comparing it to the index 1 and the index 0. For the last index n, insertion sort will make n comparisons and place the smallest value (since the array is in reverse order) in the front. This totals to (n*(n-1)/2), which gives O(n) complexity.
Their worst case asymptotic performances are equal.
In the best case asymptotic performance for sort, sort will ALSO have asymptotic complexity of O(n^2). This is because, when calculating the asymptotic performance of sort, the same logic applies regardless of the contents or ordering of the array. The guard of the for loops inside sort are independent of how each element appears to its neighbors or to any reference value; they are only broken upon reaching a certain index. In the best case, the inner for loop will perform 0 assignations and all comparisons, resulting in n-i basic operations being performed. This inner loop still retains O(n) complexity. Therefore, the best, and worst, and for that matter, every case of sort has an asymptotic complexity of O(n^2).
The best case asymptotic performance for insertion sort is when the two arrays are already sorted. In the inner for loop of the insertion sort, one of the conditions is that a[j-1] > k. The integer J always starts out with a value of i, for whichever value i represents the iteration of the outer loop that is currently being processed. K is given the value a[i]. In a sorted list, a[i-1] < a[i] for all i in the bounds of the array that have a previous element; this is the definition of a sorted list. Thus, a[i-1] < a[i] and we can substitute j for i and K for a[i] because these values represent the first iteration of the inner for loop. We can see that a[j-1] < K, which contradicts the inner for loop guard of a[j-1] > K. This means the inner for loop will never run for any i in the bounds of the array that has a previous value. The absence of a previous value is not something we have to worry about since i starts at 1. Since the inner loop never executes, the only index we observe is the index i. This means that the best case asymptotic performance for insert sort is the number of iterations of i, which runs from 1->n-1, resulting in O(n) or linear complexity. 
Insertion sort has a faster best-case asymptotic complexity.
I would expect insertion sort to be faster because in a typical randomized case, the time taken would fall somewhere between the best-case and worst-case asymptotic performances. Since sort has the same performance regardless of the orientation of the array, the asymptotic performance of sort will always be O(n^2). However, in practice, insertion sort will likely not shift every element to the first index, which would be the only instance in which it mimics O(n^2) asymptotic complexity. One of the clauses of the loop invariant of the insertion sort is that for each i, all elements to the left of i are sorted. At each iteration, insertion sort places the value at the index i at the correct spot in this sorted group. This is likely to be somewhere between the first and last indices of all elements before I, simply because placing everything at the first index would mean the array is sorted in reverse order, and placing everything in the last index would mean the array is already sorted. Since those two possibilities are 2 out of the n! possible permutations of all numbers in the array (assuming no duplicates) simple probability theory states that the array is likely to NOT be one of these two orientations, leaving insertion sort to very likely be better than sort.


2. The worst case asymptotic complexity of sort is O(n^2) as described in problem 1. The worst case asymptotic complexity of Selection sort is O(n^2) as well. This is because Selection sort, for every iteration of the for loop, checks all the values between a given index i (representing the current iteration as well as the index of the array to start at) and the last index of the array. This means that the first iteration runs through all values between index i = 0 and the last index of the array, with each subsequent iteration increasing i by 1 until i is at the last index of the array. The number of comparisons will thus be n + n-1 + n-2 + n-3... + 1, giving n(n-1)/2 which gives O(n^2) complexity.
The worst case asymptotic complexities are equal
The best case asymptotic complexity of sort is O(n^2) as described in problem 1. The best case asymptotic complexity of selection sort is O(n^2) as well. This is because, even if the minimum value is found immediately after the index i, the only way to confirm this is to check the value at each index until the very last index, which it must do for each iteration until the iteration representing the last index. This gives the same number of comparisons as the worst-case asymptotic complexity described above, and thus the mathematics will be the same. The complexity will equal O(n^2).
The best case asymptotic complexities are equal
Selection sort is expected to behave faster than sort because of the number of assignations that regular sort must perform, a number that is significantly less in selection sort. Consider one iteration of the outer for loop of the  selection sort. Selection sort assigns the variable "indexOfMinimum" to its own index. It then iterates through the rest of the array, a total of n-i-1 elements, making one comparison each iteration (we'll throw out the -1 for readability and so as to not confuse anyone). If the value at a certain index is smaller than the value at index "indexOfMinimum", "indexOfMinimum" is then assigned to this index. This could happen anywhere between 0 and n-i times. Then, the value at indexOfMinimum is swapped with the value at the original index, i. This requires 3 assignations, as swapping the values of two array indices generally do. In total, this will be anywhere between 4 and n-i + 4 assignations, and a definite n-i comparisons.
Consider one iteration of the outer for loop of the given sort. The given sort iterates through the rest of the array starting at index i+1 and ending at index n-1, for a total of n-i-1 comparisons (again throw out -1). If the value at a certain index is smaller than the value at index i, the value at index i is swapped with this value, giving 3 assignations. The most amount of times this could happen is n-i times, each giving 3 assignations for a total of 3*(n-i) assignations. Add that to the total number of comparisons and one receives 4*(n-i) fundamental operations.
Since both of these given sorts have the same number of comparisons for a given iteration of the inner and outer loops, let us consider only the fundamental operations outside the comparisons. Selection sort will only perform an assignation if the "if" statement executes, and the regular sort will perform a swap if the "if" statement executes. The base of reference for the "if" statement is the value at index i for the regular sort, and the value at index "indexOfMinimum" for the selection sort. When the if statement is executed in selection sort at an index j, selection sort changes the indexOfMinimum so that the base of reference is now the value at j. When the if statement is executed in the regular sort at an index j, the regular sort swaps the value at i with the value at j so that the value at i is now the value at j. This means that in both cases, both sorts assign their base of reference to the same number. When the if statement does not execute, there are no assignations so the base of reference never changes. At the beginning of the iteration, they are assigned to the same value since IndexOfMinimum is assigned to i. Thus, their base of reference remains equal through every iteration of the inner loop, which means that the number of times the selection sort performs an assignation is the same number as the number of times the regular sort performs a swap. This is crucial because a swap takes 3 assignations while a single assignation represents 1. However, one must note that the selection sort performs one mandatory swap at the end and a mandatory assignation at the beginning, giving it 4 extra assignations. Thus, if the "if" statement executes at least twice, the selection will be faster than the regular sort. In general, then, one can rely on the selection sort to be faster than the regular sort in array of reasonable size.

3. We start by offering a quantitative statement to represent the postcondition. Obviously, the postcondition is that the array is sorted, but this sort of statement in itself offers no direct way of evaluation. Instead, I offer this definition of sorted: for every index i in the array, if the index j is a valid index of the array and j> i, then a[i] <= a[j]. This represents a sorted array. To prove this, let us view one valid index i. We observe that this condition must hold for index i+1, if i+1 is a valid index. Then, we observe that there is no index x in the array such that a[i] < a[x] < a[j]. Under the definition I offered, we cannot place x to be before i. This is because if x < i, then a[x] <= a[i] and therefore x is not greater than i. x cannot be greater than i+1 because if x > i+1, i+1 < x, so a[i+1] <= a[x] and therefore a[x] is not less than a[i]. This means that, given an index i, we confirm that the value at index i+1 is the *next smallest* value after the value at index i. Taking each index pairwise with the index next to it confirms that the value at each index is succeeded by the next smallest value in the array.
Thus, the following is our postcondition: For every index i in the array, if the index j is a valid index of the array and j>i, then a[i] <= a[j].
To prove this, we offer two clauses
    1. 0 <= i <= n-1  (note: this is just a confirmation that i will always remain a valid index}
    2. a[x] <= a[j] if x < i <= j and x,i,j all valid indices, or either x or j is not a valid index

Establishment: clause 1 is true because at the beginning of the loop, i is given to be zero and n is assumed to be an array of at least 1 value so it has length at least 1. Clause 2 is true because x is not a valid index, which is one part of the clause. 
Preservation: Clause 1: two parts: 0<=i must be true because, in the loop, i is added to and never subtracted from. Thus, 0 <= i. Clause 2 is true because if i > n-1, then some value i2 must have been added to i to make it greater than n-1. The only values added to i are at the end of one loop iteration, at the statement i++; thus, i2 must be 1. Then, i+1 > n-1, so that means that i >= n-1. If i was ever >= n-1, then i++ would not happen because the loop guard would fail. Thus, i > n-1 is an impossibility. 
Preservation: Clause 2: to prove clause 2, I will make a digression to prove that the inner loop satisfies the post-condition a[i] <= a[i2] if i2 > i, assuming valid indices. 
    Clause 1: a[i] <= a[j-1]
    clause 2: i+1 <= j <= n
    Establishment: j is initialized as i+1 so a[i] <= a[i] must be true. also, i+1 = j must be true. Finally, j <= n because if j>n and j = i+1, then i >= n. This is impossible because we know i <= n-1 from the outer loop.
    Preservation: Clause 2: J is only ever added to, so j must only ever become greater than i+1. J <= n because if j was appended to so that j> n, the loop guard would break. Clause 1: Let us suppose the exact opposite: a[i] > a[j-1] after a certain iteration of the loop. Then, there are two possibilities. The first is that when j was equal to j-1 in the previous iteration of the loop (call this new variable j2), a[i] > a[j2] and there was no swap. This is impossible because the if-statement would be violated. It is also impossible that a[i] <= a[j2] and there WAS a swap, for the same reason: The if-statement would have been violated. Therefore, we know that at the end of the iteration j-1 (or j2), a[i] <= a[j2]. Then, we revert to iteration j. After iteration j, the only way for that statement to NOT be true is if a swap took place, otherwise, a[i] would have remained <= (a[j-1] = a[j2]). A swap will only take place if a[i] > a[j]. Then we reach our contradition. If a[i]> a[j], then a[j-1] > a[j]. Therefore, after a[i] is swapped with a[j], we can replace "a[j]" in this statement with "a[i]". This means a[j-1] > a[i]. Our initial assumption was that a[i] > a[j-1]. These two are impossible together, so we know a[i] <= a[j-1].
    Postcondition: proving that a[i] <= a[i2] if i2>i and both are valid indices. Clause 1 proves that this is true in itself for any valid J where J >= i+1. Clause 2 proves that the index j is always >= i+1 and remains a valid index until the loop guard breaks. Thus, a[i] <= a[i2] for all valid indices i, i2 where i2 > i. 
    From this nested loop invariant proof, we have shown that a[i] <= a[i2] if i2 > i and both are valid indices. Now we have to prove clause 2 of the outer loop: a[x] <= a[j] if x< i <=j. This is very easy. If, in our postcondition from the inner loop(a[i] <= a[i2] for all valid indices i, i2 where i2 > i), we let i = x and i2 = j, we have a[x] <= a[j] for all valid indices x, j, where j > x. This is pretty much the clause we are aiming to prove. we assume that we're only dealing with valid indices anyway, and x < i <= j so j must be greater than x. 
    Postcondition: Proving that for every index i in the array, if the index j is a valid index of the array and j> i, then a[i] <= a[j]. By clause 2, we know that for a given i, if we take any x< i and j>= i, both of which are valid indices, then a[x] <= a[j]. Removing i, we simply get a[x] <= a[j] where x < j. This almost works, we just need to smooth out a few bumps. Firstly, we need to confirm that i will always be a valid index, which is true by clause 1. Secondly, we note that the postcondition is actually stricter than clause 2. Clause 2 says that a[x] <= a[j] for x < i, j > x but the postcondition says a[i] <= a[j] for j > i. Clause 2 does not prove that a[x] <= a[j] for x = i; but the postcondition requies it. The solution to this is very simple. Let us say we want to prove this statement for a given index i. We simply observe an index greater than i, preferably i+1. We substitute i for x and i+1 for i in clause 2. Then, we know that for i < i+1<= j, a[i] <= a[j] and j> i. The only i without a valid "i+1" index to validate this assertion is the index i = n-1, because i+1 is the value at a[n] which is out of bounds. However, with i being the last element, we observe that any j will be out of bounds, so the post condition is still satsified. Thus, clause 2 is satisfied for all i, and i is confined to valid indices by clause 1, so for all valid indices i, if j is a valid index and j>i, then a[i] <= a[j].


